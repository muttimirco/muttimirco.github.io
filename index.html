<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mirco Mutti</title>
  
  <meta name="author" content="Mirco Mutti">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DZZCW982TZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DZZCW982TZ');
</script>

<script type="text/javascript">
function toggle_vis(id) {
    var e = document.getElementById(id);
    if (e.style.display == 'none')
    e.style.display = 'inline';
    else
    e.style.display = 'none';
}
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="files/foto.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="files/foto.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mirco Mutti</name>
              </p>
              <p> 
                I'm a <a>postdoctoral researcher</a> at the <i>Technion - Israel Intitute of Technology</i>, 
                where I work with <a href="https://avivt.github.io/avivt/">Aviv Tamar</a> in the Robot Learning Lab. 
                Previously, I received my PhD at <i>Politecnico di Milano</i> advised by <a href="https://scholar.google.com/citations?user=xdgxRiEAAAAJ">Marcello Restelli</a> within the Artificial Intelligence and Robotics Lab.
              </p>
              <p>
                My research interest is in <a>reinforcement learning</a>. 
                My current research tackles <i>generalization</i> and <a href="https://arxiv.org/pdf/2406.02282">meta RL</a> while previous contributions focused on <a href="http://amsdottorato.unibo.it/10588/1/mutti_mirco_tesi.pdf">unsupervised RL</a> or learning without rewards. 
                More broadly, my aim is to advance theoretical understanding that
                can lead to successful application of reinforcement learning in the real world.
                These include the study of <a href="https://arxiv.org/pdf/2406.12795">partial</a> <a href="https://arxiv.org/pdf/2406.02295">observability</a>, <a href="https://www.jmlr.org/papers/volume24/22-1514/22-1514.pdf">RL with general utilities</a>, <a href="https://arxiv.org/pdf/2402.03282">RLHF</a> and alternative feedback models among others.
              </p>
              <p>
                Feel free to drop an email for inquiries and questions. 
                You can find my (mostly) up to date CV below. 
                Turn to my Scholar or Twitter page to catch up with more recent agenda.
              </p>
              <p style="text-align:center">
                <a href="mailto:muttimirco@gmail.com">Email</a> &nbsp/&nbsp
                <a href="files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=GlLkJ9UAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/mirco_mutti">Twitter</a>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody><tr><td>
              <heading>News</heading>
              <ul>
                  <li>Our paper <a href="https://arxiv.org/pdf/2406.12795">The limits of pure exploration in POMDPs: When the observation entropy is enough</a> has been accepted at brand-new <a href="https://rl-conference.cc">RLC</a> conference.</li>
                  <li>Four papers accepted at ICML 2024! They cover <a href="https://arxiv.org/pdf/2406.02282">meta RL</a>, 
                    <a href="https://arxiv.org/pdf/2402.15392">inverse RL</a>, <a href="https://openreview.net/pdf?id=2JYOxcGlRe">geometric active exploration</a>, 
                    and <a href="https://arxiv.org/pdf/2406.02295">pure exploration in POMDPs</a>. See you in Vienna!</li>
                  <li>I gave a talk at the <a href="https://vandal.polito.it">VANDAL</a> lab on <i>Unsupervised reinforcement learning</i> in Turin.</li>
                  <li>Our paper <a href="https://arxiv.org/pdf/2310.07518">Exploiting causal graph priors with posterior sampling for reinforcement learning</a> has been accepted at ICLR 2024.</li>
                  <li>My PhD thesis received an honorable mention for the <i>best PhD thesis on artificial intelligence</i> by the <a href="https://aixia.it/en/">AIxIA</a></li>
                  <li>Our paper on <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/30b28eb87fe7a6c4af8520293317d4c6-Paper-Conference.pdf">Persuading farsighted receivers in MDPs: the power of honesty</a> has been accepted at NeurIPS 2023.</li>
                  <li>I gave a talk at the ETH <a href="https://las.inf.ethz.ch">LAS</a> group and <a href="https://ai.ethz.ch">AI Center</a> on <i>(Non)convex reinforcement learning</i> in Zurich.</li>
                  <li>Happy to announce I am joining the Technion as a <b><u>postdoctoral researcher</u></b> from September 2023. I will work on <i>reinforcement learning from theory to practice</i> with <a href="https://avivt.github.io/avivt/">Aviv Tamar</a>.</li>
                  <li>I succesfully defended my <a href="http://amsdottorato.unibo.it/10588/1/mutti_mirco_tesi.pdf">PhD thesis</a> on March 2023.</li>
                  <li>Our paper <a href="https://proceedings.mlr.press/v206/metelli23a/metelli23a.pdf">A tale of sampling and estimation in discounted reinforcement learning</a> has been accepted at AISTATS 2023 with an oral presentation.</li>
                  <li>I have been invited to give a talk as a <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2023">Rising stars in AI</a> at KAUST.</li>
                  <li>Our paper <a href="https://arxiv.org/pdf/2202.06545">Provably efficient causal model-based reinforcement learning for systematic generalization</a> has been accepted at AAAI 2023.</li>
                  <li>Our paper <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/1cb5b3d64bdf3c6642c8d9a8fbecd019-Paper-Conference.pdf">Challenging common assumptions in convex reinforcement learning</a> has been accepted at NeurIPS 2022</li>
                  <li>I have served as co-program chair for <a href="https://ewrl.wordpress.com/past-ewrl/ewrl15-2022/">EWRL 2022</a> we organized in Milan.</li>
                  <li>Grateful that our paper on <a href="https://proceedings.mlr.press/v162/mutti22a/mutti22a.pdf">The importance of non-Markovianity in maximum state entropy exploration</a> has been recognised with the <b><u>outstanding paper award</u></b> at ICML 2022.</li>
                  </div>
              </ul>
          </td></tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
