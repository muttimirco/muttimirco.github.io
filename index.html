<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mirco Mutti</title>
  
  <meta name="author" content="Mirco Mutti">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DZZCW982TZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DZZCW982TZ');
</script>

<script type="text/javascript">
function toggle_vis(id) {
    var e = document.getElementById(id);
    if (e.style.display == 'none')
    e.style.display = 'inline';
    else
    e.style.display = 'none';
}
</script>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="files/foto.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="files/foto.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mirco Mutti</name>
              </p>
              <p> 
                I'm a <a>postdoctoral researcher</a> at the <i>Technion - Israel Intitute of Technology</i>, 
                where I work with <a href="https://avivt.github.io/avivt/">Aviv Tamar</a> in the Robot Learning Lab. 
                Previously, I received my PhD at <i>Politecnico di Milano</i> advised by <a href="https://scholar.google.com/citations?user=xdgxRiEAAAAJ">Marcello Restelli</a> within the Artificial Intelligence and Robotics Lab.
              </p>
              <p>
                My research interest is in <a>reinforcement learning</a>. 
                My current research tackles <i>generalization</i> and <a href="https://arxiv.org/pdf/2406.02282">meta</a> <a href="https://arxiv.org/abs/2504.04505">RL</a> while previous contributions focused on <a href="http://amsdottorato.unibo.it/10588/1/mutti_mirco_tesi.pdf">unsupervised RL</a> or learning without rewards. 
                More broadly, my aim is to advance theoretical understanding that
                can lead to successful application of reinforcement learning in the real world.
                These include the study of <a href="https://arxiv.org/pdf/2406.12795">partial</a> <a href="https://arxiv.org/pdf/2406.02295">observability</a>, <a href="https://www.jmlr.org/papers/volume24/22-1514/22-1514.pdf">RL with general utilities</a>, 
                <a href="https://arxiv.org/pdf/2402.03282">RLHF</a>, <a href="https://openreview.net/forum?id=zqfT2QOiiF">imitation learning</a>, and <a href="https://arxiv.org/pdf/2402.15392">inverse</a> <a href="https://arxiv.org/pdf/2501.07996">RL</a> among others.
              </p>
              <p>
                Feel free to drop an email for inquiries and questions. 
                You can find my (mostly) up to date CV below. 
                Turn to my Scholar or Bluesky page to catch up with more recent agenda.
              </p>
              <p style="text-align:center">
                <a href="mailto:muttimirco@gmail.com">Email</a> &nbsp/&nbsp
                <a href="files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=GlLkJ9UAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://bsky.app/profile/mircomutti.bsky.social">Bluesky</a> &nbsp/&nbsp
                <a href="https://x.com/mirco_mutti">Twitter</a>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody><tr><td>
              <heading>News</heading>
              <ul>
                  <li>Our paper <a href="https://openreview.net/forum?id=zqfT2QOiiF">Blindfolded experts generalize better</a> has been awarded a best paper at the <a href="https://exait-workshop.github.io">EXAIT</a> workshop at ICML 2025.</li>
                  <li>Heading to Vancouver for ICML 2025. We are going to present <a href="https://arxiv.org/abs/2504.04505">A classification view of meta learning bandits</a> and <a href="https://arxiv.org/abs/2505.01336">Enhancing diversity in parallel agents</a> plus a pair of workshop treats.</li>
                  <li>I recently toured northern Italy (<a href="https://malga.unige.it">MaLGa</a> and <a href="https://csml.iit.it/people">IIT</a> in Genova, <a href="https://www.unibocconi.it/en">Bocconi</a>, <a href="https://www.di.univr.it">University of Verona</a>)
                  to talk about my recent works on reinforcement learning -- from theory to practice.</li>
                  <li>Our paper <a href="https://arxiv.org/pdf/2402.03282">A theoretical framework for partially-observed reward states in RLHF</a> has been accepted at ICLR 2025.</li>
                  <li>Check out our new preprint <a href="https://arxiv.org/pdf/2501.07996">Reward compatibility: A framework for inverse RL</a> 
                    to read about our journey into the theoretical foundations of inverse reinforcement learning.</li>
                  <li>Our paper <a href="https://arxiv.org/pdf/2406.03812">How does inverse RL scale to large state spaces? A provably efficient approach</a> has been accepted at NeurIPS 2024.</li>
                  <li>Our paper <a href="https://arxiv.org/pdf/2406.12795">The limits of pure exploration in POMDPs: When the observation entropy is enough</a> has been accepted at brand-new <a href="https://rl-conference.cc">RLC</a> conference.</li>
                  <li>Four papers accepted at ICML 2024. They cover <a href="https://arxiv.org/pdf/2406.02282">meta RL</a>, 
                    <a href="https://arxiv.org/pdf/2402.15392">inverse RL</a>, <a href="https://openreview.net/pdf?id=2JYOxcGlRe">geometric active exploration</a>, 
                    and <a href="https://arxiv.org/pdf/2406.02295">pure exploration in POMDPs</a>. See you in Vienna!</li>
                  <li>I gave a talk at the <a href="https://vandal.polito.it">VANDAL</a> lab on <i>Unsupervised reinforcement learning</i> in Turin.</li>
                  <li>My PhD thesis received an honorable mention for the <i>best PhD thesis on artificial intelligence</i> by the <a href="https://aixia.it/en/">AIxIA</a></li>
                  <li>I gave a talk at the ETH <a href="https://las.inf.ethz.ch">LAS</a> group and <a href="https://ai.ethz.ch">AI Center</a> on <i>(Non)convex reinforcement learning</i> in Zurich.</li>
                  <li>Happy to announce I am joining the Technion as a postdoctoral researcher from September 2023. I will work on <i>reinforcement learning from theory to practice</i> with <a href="https://avivt.github.io/avivt/">Aviv Tamar</a>.</li>
                  <li>I succesfully defended my <a href="http://amsdottorato.unibo.it/10588/1/mutti_mirco_tesi.pdf">PhD thesis</a> on March 2023.</li>
                  <li>Our paper <a href="https://proceedings.mlr.press/v206/metelli23a/metelli23a.pdf">A tale of sampling and estimation in discounted reinforcement learning</a> has been accepted at AISTATS 2023 with an oral presentation.</li>
                  <li>I have been invited to give a talk as a <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2023">Rising stars in AI</a> at KAUST.</li>
                  <li>I have served as co-program chair for <a href="https://ewrl.wordpress.com/past-ewrl/ewrl15-2022/">EWRL 2022</a> we organized in Milan.</li>
                  <li>Grateful that our paper on <a href="https://proceedings.mlr.press/v162/mutti22a/mutti22a.pdf">The importance of non-Markovianity in maximum state entropy exploration</a> has been recognised with the outstanding paper award at ICML 2022.</li>
                  </div>
              </ul>
          </td></tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
